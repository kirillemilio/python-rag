# Python-RAG: GRPC-based Embedding Retrieval Engine

This project implements a scalable, modular retrieval system using the following components:

- **NVIDIA Triton Inference Server** for embedding generation  
- **Qdrant** as a high-performance vector store  
- **gRPC** as the communication layer  
- **Configuration** via `.env` or environment variables  
- **Async Python** stack with full Prometheus observability

---

## ðŸ”§ Project Structure

```
python-rag/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ python_rag/
â”‚   â”‚   â”œâ”€â”€ server/              # gRPC server entrypoint
â”‚   â”‚   â”œâ”€â”€ proto/               # Compiled and raw proto definitions
â”‚   â”‚   â”œâ”€â”€ qdrant/              # Qdrant vector DB integration
â”‚   â”‚   â”œâ”€â”€ triton/              # Triton embedding client
â”‚   â”‚   â”œâ”€â”€ dto/                 # Data models (Chunks, Embeddings, etc.)
â”‚   â”‚   â”œâ”€â”€ config/              # Configuration logic (env, defaults, validation)
â”‚   â”‚   â”œâ”€â”€ monitoring/          # Prometheus metrics
â”‚   â”‚   â””â”€â”€ ...                  # Additional logic
â”œâ”€â”€ docker-compose.yml          # Local deployment: Qdrant + Triton
â”œâ”€â”€ .env.example                # Example environment configuration
â””â”€â”€ README.md                   # You are here
```

---

## âš™ï¸ Components

### 1. Triton Inference Server
Used to compute dense vector embeddings from input text or image. You can plug in your own model by updating the model repository path in the `.env`.

### 2. Qdrant Vector Store
Used to store and retrieve embeddings using approximate nearest neighbor (ANN) search with HNSW and optional Product Quantization (PQ).

### 3. gRPC Interface
The system is exposed **entirely via gRPC** for high-throughput, low-latency access. Clients can:
- Insert documents or chunks
- Search by embedding
- Remove entries by document or chunk ID

### 4. Configuration
Configurable via:
- `.env` file
- Or directly via environment variables

Example `.env` file:
```dotenv
TRITON_HTTP_PORT=8000
TRITON_GRPC_PORT=8001
TRITON_HOST=triton
TRITON_MODEL_NAME=mpnet

QDRANT_HOST=qdrant
QDRANT_HTTP_PORT=6333
QDRANT_GRPC_PORT=6334
QDRANT_USE_GRPC=true
QDRANT_USE_SECURE=false
QDRANT_TIMEOUT=10.0

COLLECTION_NAME=clip-image
VECTOR_SIZE=768
DISTANCE=cosine
```

---

## ðŸš€ Getting Started

### Step 1. Clone the repository

```bash
git clone https://github.com/kirillemilio/python-rag.git
cd python-rag
```

### Step 2. Setup environment

```bash
cp .env.example .env
# Then modify values as needed
```

### Step 3. Start services with Docker Compose

```bash
docker-compose up --build
```

This will start:
- `triton` on ports 8000/8001
- `qdrant` on 6333/6334

### Step 4. Run the gRPC server

```bash
python -m python_rag.server.server
```

This will initialize:
- Triton client
- Qdrant connection
- Collection creation if it doesnâ€™t exist
- Start accepting gRPC requests

---

## ðŸ“¡ gRPC API

All functionality is exposed via gRPC.

### Example Methods

- `SearchByText`: Generate embedding via Triton â†’ search in Qdrant  
- `InsertChunk`: Add a new embedding vector to collection  
- `DeleteById`: Remove a vector from Qdrant  
- `ClearCollection`: Drop all vectors

> Protobuf definitions are located in `src/python_rag/proto/` and compiled automatically.

---

## ðŸ“ˆ Observability

Prometheus metrics are exposed for:
- Embedding latency
- Search performance
- Chunk insertion stats

---

## ðŸ§ª Testing

To test functionality manually or via CI:

```bash
pytest
```

Or use a gRPC client like `grpcurl` or `grpcui`.

---

## ðŸ§  Notes

- **Batch Insertion:** Use `add_chunks_in_batches()` to optimize write throughput to Qdrant.
- **Security:** Currently no TLS. Production deployments should secure traffic.
- **Scalability:** Easily containerizable and horizontally scalable with message queues or FastAPI wrappers.

---

## ðŸ“ƒ License

MIT License. Use at your own risk. Contributions welcome.

---

## ðŸ‘¤ Author

Made by @kirillemilio â€” a lightweight async vector search engine with gRPC âœ¨.